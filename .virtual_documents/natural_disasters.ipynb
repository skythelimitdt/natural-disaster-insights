


#import Dependencies
import pandas as pd
import os
from pathlib import Path
#new libarary used
import hashlib


#load excel file 
file_to_load = Path(".", "resources","natural_disasters_data.xlsx")

#read the excel file 
natural_disasters_df = pd.read_excel(file_to_load)

#display the DataFrame
natural_disasters_df


#get info about total rows and columns
num_rows, num_columns = natural_disasters_df.shape

print(f"Total number of rows is: {num_rows}")
print(f"Total number of columns is: {num_columns}")


#gt info about data
natural_disasters_df.info()


#check for any null values
natural_disasters_df.isna().sum()


#drop column that contains all null values
natural_disasters_df = natural_disasters_df.drop(
    ["AID Contribution ('000 US$)", 
     "Reconstruction Costs, Adjusted ('000 US$)", 
     "Reconstruction Costs ('000 US$)"], 
    axis=1
)


# drop rows with na values
natural_disasters_df = natural_disasters_df.dropna(subset=['End Month'])
natural_disasters_df = natural_disasters_df.dropna(subset=['Start Day'])
natural_disasters_df = natural_disasters_df.dropna(subset=['End Day'])


natural_disasters_df.info()


#check for any duplicates
natural_disasters_df[natural_disasters_df.duplicated()]


#fetch random rows
natural_disasters_df.sample(5)


#looks at the column Disaster Type and then Total Affected within each group and aggregaes the calculations for each
summary_stats_df = natural_disasters_df.groupby("Disaster Type")["Total Affected"].agg(["min", "max", "mean", "median", "var", "std", "sem"])

summary_stats_df


#defines function for row
def create_locationid(row):
#combine multiple column headers into a unique_string, concatonate values with _
    unique_string = f"{row['ISO']}_{row['Country']}_{row['Subregion']}_{row['Region']}_{row['Location']}"
#use hashlib to create unique identifiers for LocationID 
    return hashlib.md5(unique_string.encode()).hexdigest()
#apply function to  DataFrame
natural_disasters_df['LocationID'] = natural_disasters_df.apply(create_locationid, axis=1)

natural_disasters_df.columns.tolist()


#rename column names
natural_disasters_df.rename(columns=
                            {"Total Damage ('000 US$)": "Total_Damage(USD)",
                            "Total Damage, Adjusted ('000 US$)": "Total_Damage_Adj(USD)",
                            "Classification Key": "Classification_Key",
                             "Disaster Group": "Disaster_Group",
                             "Disaster Subgroup": "Disaster_Subgroup",
                             "Disaster Type": "Disaster_Type",
                             "Disaster Subtype": "Disaster_Subtype",
                             "DisNo.": "DisNo",
                             "River Basin": "River_Basin",
                             "External IDs": "External_IDs",
                             "Associated Types":"Associated_Types",
                             "Magnitude Scale": "Magnitude_Scale",
                             "Event Name": "Event_Name",
                             "OFDA/BHA Response": "OFDA_BHA_Response",
                             "Total Deaths": "Total_Deaths",
                             "No. Injured":"No_Injured",
                             "No. Affected":"No_Affected",
                             "No. Homeless":"No_Homeless",
                             "Total Affected": "Total_Affected",
                             "Start Year": "Start_Year"
                            }, 
                            inplace=True)
natural_disasters_df.head()


#defines function for row
def create_impactid(row):
#combine multiple column headers into a unique_string, concatonate values with _
    unique_string = f"{row['DisNo']}_{row['Total_Deaths']}_{row['No_Injured']}_{row['No_Affected']}_{row['Total_Damage(USD)']}"
#use hashlib to create unique identifiers for ImpactID 
    return hashlib.md5(unique_string.encode()).hexdigest()
#apply function to  DataFrame
natural_disasters_df['ImpactID'] = natural_disasters_df.apply(create_impactid, axis=1)

natural_disasters_df.columns.tolist()


#combine date columns into single column with date format
natural_disasters_df["Start_Date"] = pd.to_datetime(
    natural_disasters_df[["Start_Year", "Start Month", "Start Day"]].rename(
        columns={"Start_Year": "year", "Start Month": "month", "Start Day": "day"}))

natural_disasters_df["End_Date"] = pd.to_datetime(
    natural_disasters_df[["End Year", "End Month", "End Day"]].rename(
        columns={"End Year": "year", "End Month": "month", "End Day": "day"}))


natural_disasters_df


#Add duration of disasters in days
natural_disasters_df["duration"] = (natural_disasters_df["End_Date"] - natural_disasters_df["Start_Date"]).dt.days
# Option 1: Replace NaN or inf with 0
natural_disasters_df["duration"] = natural_disasters_df["duration"].fillna(0)
# Convert the duration to integer
natural_disasters_df["duration"] = natural_disasters_df["duration"].astype(int)
natural_disasters_df.head()


# remove commas in Event_Name
natural_disasters_df['Event_Name'] = natural_disasters_df['Event_Name'].fillna('').str.replace(',', ' ')


natural_disasters_df.info()


natural_disasters_df.nunique()


#total deaths and injuries by disaster
total_deaths_injuries_by_disaster = (natural_disasters_df.groupby("Disaster_Type")[["Total_Deaths", "No_Injured"]]
    .sum().sort_values(by="Total_Deaths", ascending=False).reset_index())

total_deaths_injuries_by_disaster


#find disaster with highest fatalities by year
highest_fatalities_by_year = (
    #finds the index of the row with the max Total Deaths for each year
    natural_disasters_df.loc[natural_disasters_df.groupby("Start_Year")["Total_Deaths"].idxmax()]
    [["Start_Year", "Disaster_Type", "Total_Deaths", "Event_Name", "Country"]]
    #sort total deaths in descending order
    .sort_values(by="Total_Deaths", ascending=False))

highest_fatalities_by_year


#create classification dataframe
classification_df = natural_disasters_df[[
    "Classification_Key", "Disaster_Group", 
    "Disaster_Subgroup", "Disaster_Type", 
    "Disaster_Subtype"
]].drop_duplicates(subset='Classification_Key')
classification_df


#create damage DataFrame
damage_df = natural_disasters_df[["ImpactID", "DisNo", "Classification_Key", "LocationID", 
                                  "ISO", "Start_Year","Total_Damage(USD)", "Total_Damage_Adj(USD)"]]

damage_df


# create Affected dataframe
affected_df = natural_disasters_df[["ImpactID", "LocationID", "Classification_Key",
                                  "ISO", "Total_Deaths", "No_Injured", "No_Affected",
                                   "No_Homeless", "Total_Affected"]]

affected_df


#create disaster_events DataFrame
events_df = natural_disasters_df[[
    "DisNo", "ImpactID", "Historic", "Classification_Key", 
    "Disaster_Group", "Disaster_Subgroup","Disaster_Type", 
    "Disaster_Subtype", "Event_Name", "OFDA_BHA_Response",
    "Appeal", "Declaration", "Origin", 
    "Associated_Types", "Magnitude", "Magnitude_Scale", 
    "Start_Date", "End_Date", "duration"
]]

events_df.head()


#get disaster_type count
disaster_type = events_df["Disaster_Type"].value_counts()

disaster_type


#create locations DataFrame
locations_df = natural_disasters_df[[
                                     "LocationID", "DisNo", "Classification_Key",
                                     "ISO", "Country", "Subregion", "Region", 
                                     "Location", "Latitude", "Longitude", 
                                     "River_Basin"
                                    ]].drop_duplicates(subset='LocationID')

locations_df


#get disaster_type count
location_count = locations_df["Location"].value_counts()

location_count


damage_df.dtypes


events_df.dtypes


locations_df.dtypes


affected_df.dtypes


#export DataFrames as csv files
events_df.to_csv("resources/events.csv", index=False)

locations_df.to_csv("resources/locations.csv", index=False)

damage_df.to_csv("resources/damage.csv", index=False)

affected_df.to_csv("resources/affected.csv", index=False)

classification_df.to_csv("resources/classification.csv", index=False)







